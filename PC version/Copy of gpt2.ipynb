{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1XdY67GLA3YQnoOihmy7Pp2msvUcxkHPP","authorship_tag":"ABX9TyPQG7KYWBu9RPzlpWLWoNzb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Define a custom dataset class to load your text data\n","class TextDataset(Dataset):\n","    def __init__(self, file_path, tokenizer, block_size=128):\n","        self.examples = []\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            text = f.read()\n","            tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n","            for i in range(0, len(tokenized_text) - block_size + 1, block_size):  # Truncate in block of block_size\n","                self.examples.append(tokenized_text[i:i + block_size])\n","\n","    def __len__(self):\n","        return len(self.examples)\n","\n","    def __getitem__(self, idx):\n","        return torch.tensor(self.examples[idx], dtype=torch.long)\n","\n","# Load pre-trained GPT-2 model and tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","\n","# Path to your text file about human brain and function\n","data_file_path = \"/content/file.txt\"\n","\n","# Create a custom dataset instance\n","dataset = TextDataset(data_file_path, tokenizer)\n","\n","# Define training parameters\n","train_params = {\n","    \"batch_size\": 4,\n","    \"shuffle\": True,\n","    \"num_workers\": 0\n","}\n","\n","# Create a DataLoader for the dataset\n","train_loader = DataLoader(dataset, **train_params)\n","\n","# Training loop\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","model.train()\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n","for epoch in range(3):  # Train for 3 epochs, you can adjust as needed\n","    for batch in train_loader:\n","        inputs, labels = batch.to(device), batch.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n","\n","# Save the fine-tuned model\n","model.save_pretrained(\"fine_tuned_gpt_model\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n3oiAsY5d7LJ","executionInfo":{"status":"ok","timestamp":1713533540801,"user_tz":-330,"elapsed":6588,"user":{"displayName":"Akshay Krishna","userId":"12832933899904921493"}},"outputId":"b5183471-e48c-412b-aff8-9c93636031c4"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 3.5302655696868896\n","Epoch 1, Loss: 3.211371898651123\n","Epoch 1, Loss: 3.202302932739258\n","Epoch 1, Loss: 3.369623899459839\n","Epoch 1, Loss: 3.2692081928253174\n","Epoch 1, Loss: 3.261125087738037\n","Epoch 1, Loss: 3.4480910301208496\n","Epoch 2, Loss: 2.9390673637390137\n","Epoch 2, Loss: 2.933866500854492\n","Epoch 2, Loss: 2.811767816543579\n","Epoch 2, Loss: 2.784903049468994\n","Epoch 2, Loss: 2.581923246383667\n","Epoch 2, Loss: 2.7124576568603516\n","Epoch 2, Loss: 2.5476086139678955\n","Epoch 3, Loss: 2.4683334827423096\n","Epoch 3, Loss: 2.645562171936035\n","Epoch 3, Loss: 2.4801926612854004\n","Epoch 3, Loss: 2.471576452255249\n","Epoch 3, Loss: 2.453244209289551\n","Epoch 3, Loss: 2.266193389892578\n","Epoch 3, Loss: 2.5392534732818604\n"]}]},{"cell_type":"code","source":["tokenizer.save_pretrained(\"fine_tuned_gpt_model\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6DGwIoF1hBFQ","executionInfo":{"status":"ok","timestamp":1713533553436,"user_tz":-330,"elapsed":414,"user":{"displayName":"Akshay Krishna","userId":"12832933899904921493"}},"outputId":"e36c8bc3-181e-4133-f7e5-86cf647a6aae"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('fine_tuned_gpt_model/tokenizer_config.json',\n"," 'fine_tuned_gpt_model/special_tokens_map.json',\n"," 'fine_tuned_gpt_model/vocab.json',\n"," 'fine_tuned_gpt_model/merges.txt',\n"," 'fine_tuned_gpt_model/added_tokens.json')"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","\n","# Load fine-tuned model and tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained(\"fine_tuned_gpt_model\")\n","model = GPT2LMHeadModel.from_pretrained(\"fine_tuned_gpt_model\")\n","\n","# Set the model in evaluation mode\n","model.eval()\n","\n","# Prompt for completion\n","prompt_text = \"The human eye is a\"\n","\n","# Tokenize input text\n","input_ids = tokenizer.encode(prompt_text, return_tensors=\"pt\")\n","\n","# Generate text\n","# Generate text with greedy decoding\n","output_text = \"\"\n","num_sentences = 0\n","while num_sentences < 2:\n","    output = model.generate(\n","        input_ids,\n","        max_length=100,\n","        num_return_sequences=1,  # Use 1 for greedy decoding\n","        temperature=0.7,\n","        top_k=50,  # Adjust this parameter as needed for diversity in generated text\n","        top_p=0.95,  # Adjust this parameter as needed for diversity in generated text\n","        do_sample=True  # Necessary for using temperature\n","    )\n","\n","    # Decode and append generated text\n","    output_text += tokenizer.decode(output[0], skip_special_tokens=True)\n","\n","    # Count the number of sentence-ending tokens (period, question mark, exclamation mark)\n","    num_sentences += output_text.count(\".\") + output_text.count(\"?\") + output_text.count(\"!\")\n","\n","    # Update input for next generation\n","    input_ids = tokenizer.encode(output_text, return_tensors=\"pt\")\n","\n","# Print the generated text\n","print(output_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w9biNRPKd78m","executionInfo":{"status":"ok","timestamp":1713533888984,"user_tz":-330,"elapsed":8651,"user":{"displayName":"Akshay Krishna","userId":"12832933899904921493"}},"outputId":"342b9ec4-31b4-458a-a29b-83b555cfccfb"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["The human eye is a specialized organ that acts like a camera, filtering light and providing visual information to the brain. The human eye is a complex organ that acts like a camera, filtering light and providing visual information to the brain. The human eye, like any other part of the body, is composed of thousands of cells, each carrying thousands of receptors and many different types of cells.\n","\n","The human eye is the world's largest organ, and it contains thousands of cells, each carrying thousands of\n"]}]},{"cell_type":"code","source":["# Generate text with greedy decoding\n","output = model.generate(\n","    input_ids,\n","    max_length=50,  # Set maximum length to accommodate the desired length of two sentences\n","    num_return_sequences=1,  # Generate one sequence at a time\n","    temperature=0.7,\n","    top_k=50,  # Adjust this parameter as needed for diversity in generated text\n","    top_p=0.95,  # Adjust this parameter as needed for diversity in generated text\n","    do_sample=True,  # Necessary for using temperature\n","    max_new_tokens=30  # Limit the additional tokens generated beyond the input length\n",")\n","\n","# Decode and print generated text\n","for i, sample_output in enumerate(output):\n","    decoded_output = tokenizer.decode(sample_output, skip_special_tokens=True)\n","    sentences = decoded_output.split('.')\n","    if len(sentences) > 1:\n","        first_sentence = sentences[0] + '.'\n","        second_sentence = sentences[1] + '.'\n","        print(f\"Generated text {i+1}: First sentence: {first_sentence} Second sentence: {second_sentence}\\n\")\n","    else:\n","        print(\"Generated text does not contain two sentences.\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1gwV3LWHlskg","executionInfo":{"status":"ok","timestamp":1713534423059,"user_tz":-330,"elapsed":3294,"user":{"displayName":"Akshay Krishna","userId":"12832933899904921493"}},"outputId":"806a343c-1af0-4c63-f6ff-9af3d992c87c"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Both `max_new_tokens` (=30) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["Generated text 1: First sentence: The human eye is a specialized organ that acts like a camera, filtering light and providing visual information to the brain. Second sentence:  The human eye is a complex organ that acts like a camera, filtering light and providing visual information to the brain.\n","\n"]}]},{"cell_type":"code","source":["!zip '/content/fine_tuned_gpt_model'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"id8F1eGZgZog","executionInfo":{"status":"ok","timestamp":1713533074920,"user_tz":-330,"elapsed":3,"user":{"displayName":"Akshay Krishna","userId":"12832933899904921493"}},"outputId":"70838e1a-5516-4b46-ca26-fe2e6e801f32"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","zip error: Nothing to do! (/content/fine_tuned_gpt_model.zip)\n"]}]}]}